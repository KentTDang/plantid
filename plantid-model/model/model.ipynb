{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72f9375e-b697-4256-b5b1-05c1a4e488ed",
   "metadata": {},
   "source": [
    "# Loading data from preprocessed pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db562f6b-6613-458f-ad13-da925dba80c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# open the pickle file in read-binary mode\n",
    "with open('..//model-data/preprocessing/train-val-sets/train_val_split.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# storing data\n",
    "images_train = data['images_train']\n",
    "images_val = data['images_val']\n",
    "labels_train = data['labels_train']\n",
    "labels_val = data['labels_val']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5ef8ef-a00b-4d0d-82e5-3ae106517c12",
   "metadata": {},
   "source": [
    "# Creating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c012f95d-93d2-470d-85b0-e1d08adb96c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers, Sequential\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Rescaling(scale=1.0/255, input_shape=(256, 256, 3)),\n",
    "    \n",
    "    layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "    \n",
    "    layers.Dense(32, activation='relu'),\n",
    "    #layers.BatchNormalization(),\n",
    "    \n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "    \n",
    "    layers.Dense(32, activation='relu'),\n",
    "    #layers.BatchNormalization(),\n",
    "    \n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Flatten(),\n",
    "    layers.Dense(39, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42bc81eb-3d84-4430-95f1-90b7ffff9ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-26 04:45:54.911620: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-26 04:45:54.925983: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-01-26 04:45:54.946383: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-01-26 04:45:54.946419: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-26 04:45:54.959669: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/preprocessing/tf_data_layer.py:19: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "2025-01-26 04:45:56.440390: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-01-26 04:45:56.448778: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-01-26 04:45:56.451464: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-01-26 04:45:56.455538: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-01-26 04:45:56.458195: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-01-26 04:45:56.460497: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-01-26 04:45:56.588411: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-01-26 04:45:56.591109: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-01-26 04:45:56.592402: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-01-26 04:45:56.593457: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6147 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers, Sequential\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Rescaling(scale=1.0/255, input_shape=(256, 256, 3)),\n",
    "    \n",
    "    layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "    \n",
    "    layers.Dense(32, activation='relu'),\n",
    "    #layers.BatchNormalization(),\n",
    "    \n",
    "    layers.MaxPooling2D((3, 3)),\n",
    "\n",
    "    layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "    \n",
    "    layers.Dense(32, activation='relu'),\n",
    "    #layers.BatchNormalization(),\n",
    "    \n",
    "    layers.MaxPooling2D((3, 3)),\n",
    "    \n",
    "    layers.Flatten(),\n",
    "    layers.Dense(39, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c30ab9-fb5f-4918-be39-b7237d06b0ef",
   "metadata": {},
   "source": [
    "# Compling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d626e483-0ac1-4fbd-b81c-52071a3df8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b233db3f-7f10-4127-9705-b24888ad6fd1",
   "metadata": {},
   "source": [
    "# Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18bd3939-885c-4cd0-9dd3-29a8696e64a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1737866761.832586    5943 service.cc:145] XLA service 0x70da48006000 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1737866761.832623    5943 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 4070 Laptop GPU, Compute Capability 8.9\n",
      "2025-01-26 04:46:01.866971: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-01-26 04:46:02.018876: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8906\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1737866763.376111    5990 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_541', 544 bytes spill stores, 544 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  3/440\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 42ms/step - accuracy: 0.0312 - loss: 3.6154    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1737866766.442171    5943 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 42ms/step - accuracy: 0.4412 - loss: 2.0668\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 42ms/step - accuracy: 0.7754 - loss: 0.7468\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 42ms/step - accuracy: 0.8325 - loss: 0.5404\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 42ms/step - accuracy: 0.8757 - loss: 0.4027\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 42ms/step - accuracy: 0.8999 - loss: 0.3270\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 42ms/step - accuracy: 0.9085 - loss: 0.2963\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 42ms/step - accuracy: 0.9232 - loss: 0.2473\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 42ms/step - accuracy: 0.9360 - loss: 0.2058\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 42ms/step - accuracy: 0.9362 - loss: 0.2075\n"
     ]
    }
   ],
   "source": [
    "# sliding window model training (to handle batching due to tensorflow gpu mem errors)\n",
    "for w in range(3):\n",
    "    # calculating a third of the data size\n",
    "    third_of_data = len(images_train) // 3\n",
    "    HR_size = third_of_data\n",
    "    LR_size = 0\n",
    "    \n",
    "    for i in range(3):\n",
    "        images_train_third = images_train[LR_size:HR_size]\n",
    "        labels_train_third = labels_train[LR_size:HR_size]\n",
    "    \n",
    "        model.fit(images_train_third, labels_train_third, epochs=1)\n",
    "        \n",
    "        LR_size += third_of_data;\n",
    "        HR_size += third_of_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468d72bc-c6ab-4a2b-890b-e53a2322dfd4",
   "metadata": {},
   "source": [
    "# Validating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75c35cfc-fbce-4d00-b62c-6150f7504f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m331/331\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step\n",
      "Validation Accuracy: 0.9026607328851435\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# converting one hot encoded labels to class indices\n",
    "real_labels = np.argmax(labels_val, axis=1)\n",
    "\n",
    "# predict on the validation set\n",
    "predictions = model.predict(images_val)\n",
    "\n",
    "# converting predictions from one hot encoded probabiliti|es to class indices\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# comparing predictions with the real labels\n",
    "print(\"Validation Accuracy:\", accuracy_score(real_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf0cee5-773b-4142-9735-b38db42cd7e7",
   "metadata": {},
   "source": [
    "# One hot max on output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d2791f-5769-4a37-97ed-bd47e840e5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def one_hot_max(arr):\n",
    "    result = np.zeros_like(arr)\n",
    "    result[np.argmax(arr)] = 1\n",
    "    return result\n",
    "\n",
    "print(one_hot_max(predictions[20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80111a67-cfb2-41ca-94c1-86e5e3baca8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels_val[20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a83337-a3a5-452e-8640-80fceebeda3a",
   "metadata": {},
   "source": [
    "# Confidence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91836490-5fa5-4c4f-823b-ef1ae7215299",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "c = np.max(predictions[10]) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42cd861-f03f-4daa-a9f5-88f10f189051",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c9c47d-d0a5-4d31-a3ff-2271c694db90",
   "metadata": {},
   "source": [
    "# Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86cf37c3-d0ab-4d8f-9950-4e91a14f8c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "save_dir = './model-saves'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "model.save_weights(f'{save_dir}/90_percent_accuracy.weights.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
